{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbsDmcKvWVrh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml8RTXFZxPkd"
      },
      "source": [
        "## SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4o-1GNYVxWUw",
        "outputId": "f60405cf-00e5-43a0-ff03-7d7ad16556db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting validators\n",
            "  Downloading validators-0.19.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (4.4.2)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.19.0-py3-none-any.whl size=19553 sha256=5b30092e612f7268e575a657a8b2324853923b0de9001019b48880d46cb7805f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/5d/69/ff53a908b9f14fb7730a58fdede0fac4cdc99ef3624ec76d05\n",
            "Successfully built validators\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5Mme4MUCxVM8",
        "outputId": "1ea8e9d1-d925-4930-c135-802b8dfe19a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import typing\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwgbqcnBxnc9"
      },
      "source": [
        "### Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "duqhKKqbxpc9"
      },
      "outputs": [],
      "source": [
        "DEFAULT_BUILD_PARAMS = {\n",
        "    'height': 31,\n",
        "    'width': 200,\n",
        "    'color': False,\n",
        "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
        "    'rnn_units': (128, 128),\n",
        "    'dropout': 0.25,\n",
        "    'rnn_steps_to_discard': 2,\n",
        "    'pool_size': 2,\n",
        "    'stn': True,\n",
        "}\n",
        "\n",
        "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
        "\n",
        "PRETRAINED_WEIGHTS = {\n",
        "    'kurapan': {\n",
        "        'alphabet': DEFAULT_ALPHABET,\n",
        "        'build_params': DEFAULT_BUILD_PARAMS,\n",
        "        'weights': {\n",
        "            'notop': {\n",
        "                'url':\n",
        "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
        "                'filename': 'crnn_kurapan_notop.h5',\n",
        "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
        "            },\n",
        "            'top': {\n",
        "                'url':\n",
        "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
        "                'filename': 'crnn_kurapan.h5',\n",
        "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH9oviLCzOI6"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GkP9-8YZzTWh"
      },
      "outputs": [],
      "source": [
        "def swish(x, beta=1):\n",
        "    return x * keras.backend.sigmoid(beta * x)\n",
        "\n",
        "\n",
        "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
        "\n",
        "\n",
        "def _repeat(x, num_repeats):\n",
        "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
        "    x = tf.reshape(x, shape=(-1, 1))\n",
        "    x = tf.matmul(x, ones)\n",
        "    return tf.reshape(x, [-1])\n",
        "\n",
        "\n",
        "def _meshgrid(height, width):\n",
        "    x_linspace = tf.linspace(-1., 1., width)\n",
        "    y_linspace = tf.linspace(-1., 1., height)\n",
        "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
        "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
        "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
        "    ones = tf.ones_like(x_coordinates)\n",
        "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
        "    return indices_grid\n",
        "\n",
        "\n",
        "# pylint: disable=too-many-statements\n",
        "def _transform(inputs):\n",
        "    locnet_x, locnet_y = inputs\n",
        "    output_size = locnet_x.shape[1:]\n",
        "    batch_size = tf.shape(locnet_x)[0]\n",
        "    height = tf.shape(locnet_x)[1]\n",
        "    width = tf.shape(locnet_x)[2]\n",
        "    num_channels = tf.shape(locnet_x)[3]\n",
        "\n",
        "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
        "\n",
        "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
        "    locnet_y = tf.cast(locnet_y, 'float32')\n",
        "\n",
        "    output_height = output_size[0]\n",
        "    output_width = output_size[1]\n",
        "    indices_grid = _meshgrid(output_height, output_width)\n",
        "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
        "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
        "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
        "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
        "\n",
        "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
        "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
        "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
        "    x = tf.reshape(x_s, [-1])\n",
        "    y = tf.reshape(y_s, [-1])\n",
        "\n",
        "    # Interpolate\n",
        "    height_float = tf.cast(height, dtype='float32')\n",
        "    width_float = tf.cast(width, dtype='float32')\n",
        "\n",
        "    output_height = output_size[0]\n",
        "    output_width = output_size[1]\n",
        "\n",
        "    x = tf.cast(x, dtype='float32')\n",
        "    y = tf.cast(y, dtype='float32')\n",
        "    x = .5 * (x + 1.0) * width_float\n",
        "    y = .5 * (y + 1.0) * height_float\n",
        "\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    max_y = tf.cast(height - 1, dtype='int32')\n",
        "    max_x = tf.cast(width - 1, dtype='int32')\n",
        "    zero = tf.zeros([], dtype='int32')\n",
        "\n",
        "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "\n",
        "    flat_image_dimensions = width * height\n",
        "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
        "    flat_output_dimensions = output_height * output_width\n",
        "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
        "    base_y0 = base + y0 * width\n",
        "    base_y1 = base + y1 * width\n",
        "    indices_a = base_y0 + x0\n",
        "    indices_b = base_y1 + x0\n",
        "    indices_c = base_y0 + x1\n",
        "    indices_d = base_y1 + x1\n",
        "\n",
        "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
        "    flat_image = tf.cast(flat_image, dtype='float32')\n",
        "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
        "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
        "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
        "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
        "\n",
        "    x0 = tf.cast(x0, 'float32')\n",
        "    x1 = tf.cast(x1, 'float32')\n",
        "    y0 = tf.cast(y0, 'float32')\n",
        "    y1 = tf.cast(y1, 'float32')\n",
        "\n",
        "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
        "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
        "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
        "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
        "    transformed_image = tf.add_n([\n",
        "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
        "        area_d * pixel_values_d\n",
        "    ])\n",
        "    # Finished interpolation\n",
        "\n",
        "    transformed_image = tf.reshape(transformed_image,\n",
        "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
        "    return transformed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALRK3SR1x8JH"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i2YGCF1mWVrz"
      },
      "outputs": [],
      "source": [
        "def CTCDecoder():\n",
        "    def decoder(y_pred):\n",
        "        input_shape = tf.keras.backend.shape(y_pred)\n",
        "        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
        "            input_shape[1], 'float32')\n",
        "        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
        "        unpadded_shape = tf.keras.backend.shape(unpadded)\n",
        "        padded = tf.pad(unpadded,\n",
        "                        paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
        "                        constant_values=-1)\n",
        "        return padded\n",
        "\n",
        "    return tf.keras.layers.Lambda(decoder, name='decode')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DaRONZgExrQL"
      },
      "outputs": [],
      "source": [
        "def build_model(alphabet,\n",
        "                height,\n",
        "                width,\n",
        "                color,\n",
        "                filters,\n",
        "                rnn_units,\n",
        "                dropout,\n",
        "                rnn_steps_to_discard,\n",
        "                pool_size,\n",
        "                stn=True):\n",
        "    \"\"\"Build a Keras CRNN model for character recognition.\n",
        "    Args:\n",
        "        height: The height of cropped images\n",
        "        width: The width of cropped images\n",
        "        color: Whether the inputs should be in color (RGB)\n",
        "        filters: The number of filters to use for each of the 7 convolutional layers\n",
        "        rnn_units: The number of units for each of the RNN layers\n",
        "        dropout: The dropout to use for the final layer\n",
        "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
        "        pool_size: The size of the pooling steps\n",
        "        stn: Whether to add a Spatial Transformer layer\n",
        "    \"\"\"\n",
        "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
        "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
        "    inputs = keras.layers.Input((height, width, 3 if color else 1), name='input', batch_size=1)\n",
        "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
        "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
        "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
        "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
        "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
        "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
        "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
        "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
        "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
        "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
        "    if stn:\n",
        "        # pylint: disable=pointless-string-statement\n",
        "        \"\"\"Spatial Transformer Layer\n",
        "        Implements a spatial transformer layer as described in [1]_.\n",
        "        Borrowed from [2]_:\n",
        "        downsample_fator : float\n",
        "            A value of 1 will keep the orignal size of the image.\n",
        "            Values larger than 1 will down sample the image. Values below 1 will\n",
        "            upsample the image.\n",
        "            example image: height= 100, width = 200\n",
        "            downsample_factor = 2\n",
        "            output image will then be 50, 100\n",
        "        References\n",
        "        ----------\n",
        "        .. [1]  Spatial Transformer Networks\n",
        "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
        "                Submitted on 5 Jun 2015\n",
        "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
        "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
        "        \"\"\"\n",
        "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
        "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
        "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
        "                                       activation='relu')(stn_input_layer)\n",
        "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
        "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
        "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
        "        locnet_y = keras.layers.Dense(6,\n",
        "                                      weights=[\n",
        "                                          np.zeros((64, 6), dtype='float32'),\n",
        "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
        "                                      ])(locnet_y)\n",
        "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
        "        x = keras.layers.Lambda(_transform,\n",
        "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
        "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
        "                                           (height // pool_size**2) * filters[-1]),\n",
        "                             name='reshape')(x)\n",
        "\n",
        "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
        "\n",
        "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
        "                                      kernel_initializer=\"he_normal\",\n",
        "                                      return_sequences=True,\n",
        "                                      name='lstm_10')(x)\n",
        "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
        "                                   kernel_initializer=\"he_normal\",\n",
        "                                   go_backwards=True,\n",
        "                                   return_sequences=True,\n",
        "                                   name='lstm_10_back')(x)\n",
        "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
        "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
        "                                      kernel_initializer=\"he_normal\",\n",
        "                                      return_sequences=True,\n",
        "                                      name='lstm_11')(rnn_1_add)\n",
        "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
        "                                   kernel_initializer=\"he_normal\",\n",
        "                                   go_backwards=True,\n",
        "                                   return_sequences=True,\n",
        "                                   name='lstm_11_back')(rnn_1_add)\n",
        "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
        "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
        "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
        "    x = keras.layers.Dense(len(alphabet) + 1,\n",
        "                           kernel_initializer='he_normal',\n",
        "                           activation='softmax',\n",
        "                           name='fc_12')(x)\n",
        "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
        "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
        "    prediction_model = keras.models.Model(inputs=inputs, outputs=CTCDecoder()(model.output))\n",
        "    return model, prediction_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NPBjgj6gyECQ"
      },
      "outputs": [],
      "source": [
        "build_params = DEFAULT_BUILD_PARAMS\n",
        "alphabets = DEFAULT_ALPHABET\n",
        "blank_index = len(alphabets)\n",
        "\n",
        "model, prediction_model = build_model(alphabet=alphabets, **build_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLVl95alzwI8"
      },
      "source": [
        "## Download and Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dmBRQeL90HoQ"
      },
      "outputs": [],
      "source": [
        "def get_default_cache_dir():\n",
        "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
        "                                                                              '.keras-ocr')))\n",
        "def sha256sum(filename):\n",
        "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
        "    h = hashlib.sha256()\n",
        "    b = bytearray(128 * 1024)\n",
        "    mv = memoryview(b)\n",
        "    with open(filename, 'rb', buffering=0) as f:\n",
        "        for n in iter(lambda: f.readinto(mv), 0):\n",
        "            h.update(mv[:n])\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
        "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
        "    hash.\n",
        "    Args:\n",
        "        url: The file to download\n",
        "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
        "            matches, we don't download it again.\n",
        "        cache_dir: The directory in which to cache the file. The default is\n",
        "            `~/.keras-ocr`.\n",
        "        verbose: Whether to log progress\n",
        "        filename: The filename to use for the file. By default, the filename is\n",
        "            derived from the URL.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = get_default_cache_dir()\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
        "    filepath = os.path.join(cache_dir, filename)\n",
        "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
        "    if verbose:\n",
        "        print('Looking for ' + filepath)\n",
        "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
        "        if verbose:\n",
        "            print('Downloading ' + filepath)\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
        "    return filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5lNpYwczLuO",
        "outputId": "587054b0-8c76-42de-d4fd-e3894b1b0bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n"
          ]
        }
      ],
      "source": [
        "weights_dict = PRETRAINED_WEIGHTS['kurapan']\n",
        "\n",
        "model.load_weights(download_and_verify(url=weights_dict['weights']['top']['url'],\n",
        "                                       filename=weights_dict['weights']['top']['filename'],\n",
        "                                       sha256=weights_dict['weights']['top']['sha256']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQXrbwF1rWO"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CUyx8EAV0IjX",
        "outputId": "8c3a4951-1ec7-4c38-dd3c-2a3d6eadde98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(1, 31, 200, 1)]    0           []                               \n",
            "                                                                                                  \n",
            " permute (Permute)              (1, 200, 31, 1)      0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (1, 200, 31, 1)      0           ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " conv_1 (Conv2D)                (1, 200, 31, 64)     640         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " conv_2 (Conv2D)                (1, 200, 31, 128)    73856       ['conv_1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv_3 (Conv2D)                (1, 200, 31, 256)    295168      ['conv_2[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_3 (BatchNormalization)      (1, 200, 31, 256)    1024        ['conv_3[0][0]']                 \n",
            "                                                                                                  \n",
            " maxpool_3 (MaxPooling2D)       (1, 100, 15, 256)    0           ['bn_3[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_4 (Conv2D)                (1, 100, 15, 256)    590080      ['maxpool_3[0][0]']              \n",
            "                                                                                                  \n",
            " conv_5 (Conv2D)                (1, 100, 15, 512)    1180160     ['conv_4[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_5 (BatchNormalization)      (1, 100, 15, 512)    2048        ['conv_5[0][0]']                 \n",
            "                                                                                                  \n",
            " maxpool_5 (MaxPooling2D)       (1, 50, 7, 512)      0           ['bn_5[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_6 (Conv2D)                (1, 50, 7, 512)      2359808     ['maxpool_5[0][0]']              \n",
            "                                                                                                  \n",
            " conv_7 (Conv2D)                (1, 50, 7, 512)      2359808     ['conv_6[0][0]']                 \n",
            "                                                                                                  \n",
            " bn_7 (BatchNormalization)      (1, 50, 7, 512)      2048        ['conv_7[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 6)            934902      ['bn_7[0][0]']                   \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (1, 50, 7, 512)      0           ['bn_7[0][0]',                   \n",
            "                                                                  'model[0][0]']                  \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (1, 50, 3584)        0           ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " fc_9 (Dense)                   (1, 50, 128)         458880      ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)                 (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
            "                                                                                                  \n",
            " lstm_10_back (LSTM)            (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
            "                                                                                                  \n",
            " add (Add)                      (1, 50, 128)         0           ['lstm_10[0][0]',                \n",
            "                                                                  'lstm_10_back[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)                 (1, 50, 128)         131584      ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " lstm_11_back (LSTM)            (1, 50, 128)         131584      ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (1, 50, 256)         0           ['lstm_11[0][0]',                \n",
            "                                                                  'lstm_11_back[0][0]']           \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (1, 50, 256)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " fc_12 (Dense)                  (1, 50, 37)          9509        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (1, 48, 37)          0           ['fc_12[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,794,267\n",
            "Trainable params: 8,791,707\n",
            "Non-trainable params: 2,560\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dg50iNc106w"
      },
      "source": [
        "## Convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y9sC2rpTe-bX",
        "outputId": "0e77015a-4bf6-4463-c0ae-2a7e32495ba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  represent_data.zip\n",
            "   creating: represent_data/\n",
            "  inflating: represent_data/word_1.png  \n",
            "  inflating: represent_data/word_2.png  \n",
            "  inflating: represent_data/word_3.png  \n",
            "  inflating: represent_data/word_4.png  \n",
            "  inflating: represent_data/word_5.png  \n",
            "  inflating: represent_data/word_6.png  \n",
            "  inflating: represent_data/word_7.png  \n",
            "  inflating: represent_data/word_8.png  \n",
            "  inflating: represent_data/word_9.png  \n",
            "  inflating: represent_data/word_10.png  \n",
            "  inflating: represent_data/word_11.png  \n",
            "  inflating: represent_data/word_12.png  \n",
            "  inflating: represent_data/word_13.png  \n",
            "  inflating: represent_data/word_14.png  \n",
            "  inflating: represent_data/word_15.png  \n",
            "  inflating: represent_data/word_16.png  \n",
            "  inflating: represent_data/word_17.png  \n",
            "  inflating: represent_data/word_18.png  \n",
            "  inflating: represent_data/word_19.png  \n",
            "  inflating: represent_data/word_20.png  \n",
            "  inflating: represent_data/word_21.png  \n",
            "  inflating: represent_data/word_22.png  \n",
            "  inflating: represent_data/word_23.png  \n",
            "  inflating: represent_data/word_24.png  \n",
            "  inflating: represent_data/word_25.png  \n",
            "  inflating: represent_data/word_26.png  \n",
            "  inflating: represent_data/word_27.png  \n",
            "  inflating: represent_data/word_28.png  \n",
            "  inflating: represent_data/word_29.png  \n",
            "  inflating: represent_data/word_30.png  \n",
            "  inflating: represent_data/word_31.png  \n",
            "  inflating: represent_data/word_32.png  \n",
            "  inflating: represent_data/word_33.png  \n",
            "  inflating: represent_data/word_34.png  \n",
            "  inflating: represent_data/word_35.png  \n",
            "  inflating: represent_data/word_36.png  \n",
            "  inflating: represent_data/word_37.png  \n",
            "  inflating: represent_data/word_38.png  \n",
            "  inflating: represent_data/word_39.png  \n",
            "  inflating: represent_data/word_40.png  \n",
            "  inflating: represent_data/word_41.png  \n",
            "  inflating: represent_data/word_42.png  \n",
            "  inflating: represent_data/word_43.png  \n",
            "  inflating: represent_data/word_44.png  \n",
            "  inflating: represent_data/word_45.png  \n",
            "  inflating: represent_data/word_46.png  \n",
            "  inflating: represent_data/word_47.png  \n",
            "  inflating: represent_data/word_48.png  \n",
            "  inflating: represent_data/word_49.png  \n",
            "  inflating: represent_data/word_50.png  \n",
            "  inflating: represent_data/word_51.png  \n",
            "  inflating: represent_data/word_52.png  \n",
            "  inflating: represent_data/word_53.png  \n",
            "  inflating: represent_data/word_54.png  \n",
            "  inflating: represent_data/word_55.png  \n",
            "  inflating: represent_data/word_56.png  \n",
            "  inflating: represent_data/word_57.png  \n",
            "  inflating: represent_data/word_58.png  \n",
            "  inflating: represent_data/word_59.png  \n",
            "  inflating: represent_data/word_60.png  \n",
            "  inflating: represent_data/word_61.png  \n",
            "  inflating: represent_data/word_62.png  \n",
            "  inflating: represent_data/word_63.png  \n",
            "  inflating: represent_data/word_64.png  \n",
            "  inflating: represent_data/word_65.png  \n",
            "  inflating: represent_data/word_66.png  \n",
            "  inflating: represent_data/word_67.png  \n",
            "  inflating: represent_data/word_68.png  \n",
            "  inflating: represent_data/word_69.png  \n",
            "  inflating: represent_data/word_70.png  \n",
            "  inflating: represent_data/word_71.png  \n",
            "  inflating: represent_data/word_72.png  \n",
            "  inflating: represent_data/word_73.png  \n",
            "  inflating: represent_data/word_74.png  \n",
            "  inflating: represent_data/word_75.png  \n",
            "  inflating: represent_data/word_76.png  \n",
            "  inflating: represent_data/word_77.png  \n",
            "  inflating: represent_data/word_78.png  \n",
            "  inflating: represent_data/word_79.png  \n",
            "  inflating: represent_data/word_80.png  \n",
            "  inflating: represent_data/word_81.png  \n",
            "  inflating: represent_data/word_82.png  \n",
            "  inflating: represent_data/word_83.png  \n",
            "  inflating: represent_data/word_84.png  \n",
            "  inflating: represent_data/word_85.png  \n",
            "  inflating: represent_data/word_86.png  \n",
            "  inflating: represent_data/word_87.png  \n",
            "  inflating: represent_data/word_88.png  \n",
            "  inflating: represent_data/word_89.png  \n",
            "  inflating: represent_data/word_90.png  \n",
            "  inflating: represent_data/word_91.png  \n",
            "  inflating: represent_data/word_92.png  \n",
            "  inflating: represent_data/word_93.png  \n",
            "  inflating: represent_data/word_94.png  \n",
            "  inflating: represent_data/word_95.png  \n",
            "  inflating: represent_data/word_96.png  \n",
            "  inflating: represent_data/word_97.png  \n",
            "  inflating: represent_data/word_98.png  \n",
            "  inflating: represent_data/word_99.png  \n",
            "  inflating: represent_data/word_100.png  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-06-02 04:02:40--  https://github.com/tulasiram58827/ocr_tflite/raw/main/data/represent_data.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tulasiram58827/ocr_tflite/main/data/represent_data.zip [following]\n",
            "--2022-06-02 04:02:40--  https://raw.githubusercontent.com/tulasiram58827/ocr_tflite/main/data/represent_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585568 (572K) [application/zip]\n",
            "Saving to: ‘represent_data.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  8% 11.6M 0s\n",
            "    50K .......... .......... .......... .......... .......... 17% 11.6M 0s\n",
            "   100K .......... .......... .......... .......... .......... 26% 15.5M 0s\n",
            "   150K .......... .......... .......... .......... .......... 34% 51.5M 0s\n",
            "   200K .......... .......... .......... .......... .......... 43% 81.8M 0s\n",
            "   250K .......... .......... .......... .......... .......... 52% 63.2M 0s\n",
            "   300K .......... .......... .......... .......... .......... 61% 22.6M 0s\n",
            "   350K .......... .......... .......... .......... .......... 69% 61.6M 0s\n",
            "   400K .......... .......... .......... .......... .......... 78% 55.5M 0s\n",
            "   450K .......... .......... .......... .......... .......... 87% 67.3M 0s\n",
            "   500K .......... .......... .......... .......... .......... 96%  209M 0s\n",
            "   550K .......... .......... .                               100%  208M=0.02s\n",
            "\n",
            "2022-06-02 04:02:40 (29.7 MB/s) - ‘represent_data.zip’ saved [585568/585568]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and unzipping representative dataset\n",
        "%%bash\n",
        "wget https://github.com/tulasiram58827/ocr_tflite/raw/main/data/represent_data.zip\n",
        "unzip represent_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-vYzKpI1fB27"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/represent_data/'\n",
        "def representative_data_gen():\n",
        "    for file in os.listdir(dataset_path):\n",
        "        image_path = dataset_path + file\n",
        "        input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        input_data = cv2.resize(input_data, (200, 31))\n",
        "        input_data = input_data[np.newaxis]\n",
        "        input_data = np.expand_dims(input_data, 3)\n",
        "        input_data = input_data.astype('float32')/255\n",
        "        yield [input_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VzyIgV-NfEeb"
      },
      "outputs": [],
      "source": [
        "def convert_tflite(quantization):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "    if quantization == 'float16':\n",
        "        converter.target_spec.supported_types = [tf.float16]\n",
        "    elif quantization == 'int8' or quantization == 'full_int8':\n",
        "        converter.representative_dataset = representative_data_gen\n",
        "    if quantization == 'full_int8':\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "        converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "    tf_lite_model = converter.convert()\n",
        "    open(f'ocr_{quantization}.tflite', 'wb').write(tf_lite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_cHVmRAfMda"
      },
      "source": [
        "**Note** : Support for CTC Decoder is not available in TFLite yet. So while converting we removed CTCDecoder in model part. We need to run Decoder from the output of the model. \n",
        "\n",
        "Refer to this [issue](https://github.com/tensorflow/tensorflow/issues/33494) regarding CTC decoder support in TFLite. \n",
        "\n",
        "**Update** : CTC Decoder is supported in TFLite now by enabling Built-in-Ops in Tensorflow 2.4. Thanks to TensorFlow team for the support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN216EKBfLt7",
        "outputId": "32b14835-843b-47d3-8973-9a11825d5327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpo2o9sy3w/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpo2o9sy3w/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'float16' #@param [\"dr\", \"float16\"]\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhWmneGKfQpl",
        "outputId": "ec096f87-9522-4746-8bcb-8d7b753de48a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du: cannot access 'ocr_dr.tflite': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!du -sh ocr_dr.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2EWZDvpfQrY",
        "outputId": "97693289-f67c-45e6-a6e7-d5f1060b18b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmphtojm5i3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmphtojm5i3/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'float16' #@param [\"dr\", \"float16\"]\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqOjRGHmfYHX",
        "outputId": "f769880f-06fc-495b-9f5e-bc91a4c7b5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17M\tocr_float16.tflite\n"
          ]
        }
      ],
      "source": [
        "!du -sh ocr_float16.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SohE_ShAfceP",
        "outputId": "febf1b12-44f9-4316-fa2f-1b8f6211689d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmplx9x952g/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmplx9x952g/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "quantization = 'int8'  #@param [\"dr\", \"float16\", 'int8', 'full_int8']\n",
        "convert_tflite(quantization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5pmotq_fkpG"
      },
      "source": [
        "**Currently Integer Quantization is erroring out and informed to TFLite team**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVhhiWe1fzBz"
      },
      "source": [
        "## TFLite Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rkGs7BDWf6dm"
      },
      "outputs": [],
      "source": [
        "def run_tflite_model(image_path, quantization):\n",
        "    input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    input_data = cv2.resize(input_data, (200, 31))\n",
        "    input_data = input_data[np.newaxis]\n",
        "    input_data = np.expand_dims(input_data, 3)\n",
        "    input_data = input_data.astype('float32')/255\n",
        "    path = f'ocr_{quantization}.tflite'\n",
        "    interpreter = tf.lite.Interpreter(model_path=path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    input_shape = input_details[0]['shape']\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DGmtdkD7gAVH"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "image_path = '/content/represent_data/word_1.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v6K9tv0u1umk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "48ff358e-1b35-457a-a821-845fd8981238"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-fdf6e1f01514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running Dynamic Range Quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtflite_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_tflite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtflite_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblank_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-01e99c82aec7>\u001b[0m in \u001b[0;36mrun_tflite_model\u001b[0;34m(image_path, quantization)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'ocr_{quantization}.tflite'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    456\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[1;32m    457\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[1;32m    459\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to open {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open 'ocr_dr.tflite'."
          ]
        }
      ],
      "source": [
        "# Running Dynamic Range Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'dr')\n",
        "final_output = \"\".join(alphabets[index] for index in tflite_output[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "DPsb744KgGqw",
        "outputId": "2269be40-20d9-4983-e8f9-03090089ac44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fyrins\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=40x23 at 0x7FEABF600A50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Running Float16 Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'float16')\n",
        "final_output = \"\".join(alphabets[index] for index in tflite_output[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kyBwZRO3gPH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "10ef9b8a-3d37-4f4e-86f7-5d39706c0617"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-691fbd163b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running Integer Quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtflite_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_tflite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblank_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoded' is not defined"
          ]
        }
      ],
      "source": [
        "# Running Integer Quantization\n",
        "tflite_output = run_tflite_model(image_path, 'int8')\n",
        "final_output = \"\".join(alphabets[index] for index in decoded[0] if index not in [blank_index, -1])\n",
        "print(final_output)\n",
        "cv2_imshow(cv2.imread(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReJFMVEjgUKj"
      },
      "source": [
        "**The above code snippet will error out as informed integer quantization is not yet supported**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4Ro3Z-gfjK"
      },
      "source": [
        "## Dynamic Range Model benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5wQBSggfXg"
      },
      "source": [
        "**Inference Time** : 0.2sec\n",
        "\n",
        "**Memory FootPrint** : 46.38MB\n",
        "\n",
        "**Model Size** : 8.5MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLqDwjRzgm_8"
      },
      "source": [
        "## Float16 benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1181AuvDgm1V"
      },
      "source": [
        "**Inference** : 0.76sec\n",
        "\n",
        "**Memory FootPrint** : 128MB\n",
        "\n",
        "**Model Size** : 17MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g_bWIW5gvni"
      },
      "source": [
        "**The above benchmarks with respect to Redmi K20 Pro with 4 threads. **"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Salinan dari KERAS_OCR_TFLITE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}